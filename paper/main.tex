\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{Применение анализа формальных понятий для исследования нейросетей}

\author{ Черемискин Егор Андреевич \\
	ММП ВМК МГУ \\
	\texttt{egorcheremiskin@yandex.ru} \\
    \And
	Гуров Сергей Исаевич\\
	ММП ВМК МГУ\\
	\texttt{sgur@cs.msu.ru} \\
}
\date{2023}

\renewcommand{\shorttitle}{Применение анализа формальных понятий для исследования нейросетей}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Применение анализа формальных понятий для исследования нейросетей},
pdfsubject={...},
pdfauthor={Черемискин Егор Андреевич, Гуров Сергей Исаевич},
pdfkeywords={Анализ формальных понятий},
}

\begin{document}

\maketitle

\begin{abstract}

Нейронные сети достигают высоких результатов в разных задачах машинного обучения, при этом зачастую возникает вопрос их интерпретируемости. В данной работе мы постараемся попробовать исследовать нейронные сети, используя анализ формальных понятий. Мы рассмотрим "схожесть" нейросетей и попробуем сравнить их, а также попробуем построить иерархию классов в задаче классификации. Это поможет нам лучше понимать работу нейросетей, а также правильно выбирать подходящую архитектуру нейросети для той или иной задачи.
 
\end{abstract}

\keywords{Анализ формальных понятий}

\section{Введение}

Применить анализ формальных понятий для исследования или улучшения работы алгоритмов машинного обучения уже пробовали другие исследователи. В работе \citep{r1} анализ формальных понятий используется для масштабирования данных. В работе \citep{r2} исследуется вопрос масштабирования данных с помощью АФП и применение масштабирования для улучшения работы древовидных классификаторов. В работах \citep{r3} и \citep{r9} авторы используют АФП для исследования интепретируемости работы древовидных классификаторов. В работе \citep{r6} АФП используется для уменьшения размерности данных. В работе \citep{r4} и \citep{r5} авторы применяют АФП для исследования нейросетей и их интерпретируемости. В работе \citep{r7} АФП используется для изучения изображений и влияние входных изображений на выход нейросети. В работе \citep{r8} АФП используется для изучения нейросетей, который используются для распознавания изображений. 

\section{Постановка задачи}

Рассмотрим нейронные сети как сложные функции от объектов обучающей выборки. Тогда, вводя некоторую метрику и используя АФП, появляется возможность сравнивать нейронные сети. Для сравнения нейронных сетей были взяты веса последнего линейного выходного слоя нейронной сети и активации нейронной сети на объектах обучающей выборки. В качестве метрики можно взять расстояние Громова-Вассерштейна. Для построения иерархии классов также будем использовать веса линейного выходного слоя и некоторый порог, в нашем случае - нулевой. 

Рассмотрим задачу классификации, где каждый признак объектов является бинарным. Любой небинарный признак можно свести к нескольким бинарным признакам: для категориального признака достаточно завести для каждой категории или для некоторых множеств категорий новый признак, для вещественного признака достаточно ввести порог, или же разбить вещественную прямую на отрезки, где каждому отрезку будет соответствовать свой новый бинарный признак.

Пусть $G$ - множество объектов, $M$ - множество бинарных признаков, а $I \subseteq G \times M$ - отношение, которое показывает, какие объекты какими признаками обладают. Тогда назовем тройку $(G, M, I)$ \emph{контекстом}. Введем операторы Галуа для подмножеств этих множеств: 

$$A' = \{\ m \in M \: | \: gIm \: \forall g \in A\}\ $$

$$B' = \{\ g \in G \: | \: gIm \: \forall m \in B\}\ $$

Если $A' = B$ и $B' = A$, то $(A, B)$ называется формальным понятием, где $A \in G, B \in M$. Множество $A$ называется \emph{формальным объемом}, а множество $B$ - \emph{формальным содержанием}. 

Если для двух понятий $(A_1, B_1)$ и $(A_2, B_2)$ выполняется $A_1 \subseteq A_2$ (или $B_2 \subseteq B_1$, что аналогично), то $(A_1, B_1)$ является \emph{подпонятием} $(A_2, B_2)$. Тогда на множестве понятий можно ввести отношение $(A_1, B_1) \leq (A_2, B_2)$. Упорядоченное множество всех формальных понятий является \emph{решеткой формальных понятий} \citep{1}. 

Пусть теперь у нас признаки являются не бинарными, а категориальными, то есть могут принимать несколько отдельных значений. Пусть $W$ - множество всевозможных значений, которые могут принимать признаки из $M$. Тогда назовем четверку $(G, M, W, I)$ \emph{многозначным контекстом}, где $I \subseteq G \times M \times W$, причем $(g, m, w) \in I$ и $(g, m, v) \in I$ тогда и только тогда, когда $w = v$. 

Для того, чтобы построить формальные понятия для многозначного контекста, необходимо применить масштабирование: по некоторым правилам построить новый контекст $S_m = (G_m, M_m, I_m)$ для некоторого признака $m$. В качестве правила может служить, например, пороговое значение для признака $m$, тем самым признак $m$ станет бинарным и по нему можно будет построить новый контекст \citep{2}. 

Рассмотрим некоторую нейронную сеть, решающую задачу бинарной классификации. У такой нейронной сети на выходе появляется вектор $[0, 1]^k$, где $k$ - число классов. На ее вход подается объект $g$ из обучающей выборки $G$ размера $n$. Признаковое описание объекта имеет размер $m$: $g \in \mathbb{R}^m$. Представим всю нейронную сеть, кроме последнего выходного слоя, как вектор функций размера $h$ $N = \{ n_1, ..., n_h \}$, где $n_i: \mathbb{R}^m \rightarrow \mathbb{R}$ для каждой функции из вектора $N$. Последний выходной слой нейронной сети представим как вектор функций $C = \{ c_1, ..., c_k \}$, где $c_i: \mathbb{R}^h \rightarrow \mathbb{R}$, то есть за каждый класс отвечает своя отдельная функция $c_i$, которая получает на вход все выходные значения скрытых слоев нейросети и возвращает вероятность объекта принадлежать данному классу. 

Пусть для построенной сложной функции нам дана матрица $\mathbb{W}$ размера $h \times k$, где каждый элемент $w_{i, j}$ отвечает за вес между нейронами $n_i$ и $c_j$. Пусть также нам дана матрица $\mathbb{O}$ размера $n \times h$, где каждый элемент $o_{i, j}$ равен $n_j(g_i)$, то есть равен активации нейрона $n_j$ для объекта $g_i$ обучающей выборки. Тогда назовем пару $(\mathbb{O}, \mathbb{W})$ многозначным представлением понятий.

Таким образом, мы можем представить выход нейронной сети как $$\mathbb{O}(g) \cdot \mathbb{W}(c) + b,$$ где $b$ - некоторое смещение. Перепишем эту формулу в следующем виде: $$|\mathbb{O}(g)| \cdot |\mathbb{W}(c)| \cdot \cos{(\mathbb{O}(g), \mathbb{W}(c))} + b,$$ для нее нам необходимо посчитать косинус угла между $\mathbb{O}(g)$ и $\mathbb{W}(c)$. 

Таким образом, мы можем рассматривать объекты обучающей выборки и классы в одном пространстве, и рассматривать задачу классификации как нахождение минимального расстояния между объектом и классами в этом пространстве. То есть мы можем ввести некоторую функцию расстояния между объектами и классами $d: G \times C \rightarrow \mathbb{R}$. В качестве такой функции может быть евклидово или косинусное расстояния. Задачу классификации в такое случае можно решать с помощью алгоритма KNN с одним ближайшим соседом, то есть для каждого объекта тестовой выборки будет находиться ближайший сосед, который является классом, и объекту тестовой выборки будет приписываться этот класс \citep{3}.

Более того, используя многозначные представления понятий нейросетей, можно сравнивать сами нейросети. Для сравнения удобно использовать расстояние Громова-Вассерштейна. Оно считается для двух матриц. В качествве этих матриц можно взять как две матрицы $\mathbb{O}$ двух нейросетей, так и две матрицы $\mathbb{W}$ этих нейросетей. Для того, чтобы посчитать расстояние Громова-Вассерштейна, строятся две матрицы, полученные из двух исходных матриц подсчетом попарного евклидово расстония для каждой матрицы и нормированные на максимальный элемент построенных матриц. Обозначим эти матрицы $C_1$ и $C_2$ для исходных двух матриц. Тогда расстояние Громова-Вассерштейна считается по следующей формуле \citep{4}:

$$GW = \min_T \sum_{i, j, k, l} L(C_{1, i, k}, C_{2, j, l}) T_{i, j} T_{k, l}, $$

где $L$ - квадратичная функция потерь, $T$ - обучаемый параметр в оптимизационной задаче, по которому находится минимум функционала. Нижняя индексация для матриц обозначает элемент матрицы, то есть $T_{i, j}$ обозначает элемент матрицы $T$ в строке $i$ и столбце $j$.

Тем самым для каждой пары нейросетей строится их многозначные представление понятий и считается их расстрояние Громова-Вассерштейна по описанной выше формуле.

Сравнивать нейросети можно и по-другому. Пусть у нас есть многозначное представление понятий $(\mathbb{O}, \mathbb{W})$ и некоторые пороговые значения $\delta_\mathbb{O}$ и $\delta_\mathbb{W}$. Тогда построим формальные контексты $\mathbb{O}_\delta$ и $\mathbb{W}_\delta$:

$$ \mathbb{O}_\delta = (G, N, I_\mathbb{O}): \: (g_i, n_j) \in I_\mathbb{O} \iff n_j(g_i) > \delta_\mathbb{O}$$

$$ \mathbb{W}_\delta = (C, N, I_\mathbb{W}): \: (c_i, n_j) \in I_\mathbb{W} \iff w_{i, j} > \delta_\mathbb{W}$$

Эти контексты можно использовать как матрицы, тогда способ сравнения будет аналогичен предыдущему, за тем исключением, что матрицы теперь бинарные, либо же использовать их для построения решетки формальных понятий, так как теперь матрица явлется масштабированной и по ней можно построить контекст. Используя решетку формальных понятий, можно построить иерархию классов, или же использовать решетки каким-либо другим способом для сравнения классов или самих нейросетей.

\section{Эксперименты}

В качестве первого эксперимента проверим состоятельность нашей идеи, то есть рассмотрим объекты обучающей выборки и классы в задаче классификации в одном пространстве, и будем классифицировать объекты с помощью алгоритма KNN с одним ближайшим соседом. Для тестирования был взят датасет ImageNet \citep{dataset}, который содержит 1000 классов изображений по 100 изображений на каждый класс. Были взяты веса тех предобученных нейронных сетей, которые решают задачу классификации изображений. В качестве расстояний брались евклидово и косинусное расстояния. В качестве метрики бралась точность (accuracy), то есть мы считаем, сколько совпало объектов тестовой выборки, для которых и нейронная сеть, и KNN предсказали один и тот же класс, и делим это число на все количество объектов тестовой выборки.Результат для трех предобученных нейронных сетей можно увидеть ниже, полностью результаты приведены в приложении.

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Нейронная сеть & Евклидово расстояние & Косинусное расстояние \\ 
 \hline
 \hline
 VGG16 & 0.94 & 0.84 \\
 \hline
 MobileNetV2 & 0.93 & 0.92 \\
 \hline
 ResNet50 & 0.96 & 0.82 \\
 \hline
\end{tabular}
\end{center}

Как видно из таблицы, описанный выше метод действительно позволяет, зная веса модели, достаточно точно проводить классификацию объектов (учитывая, что всего в датасете 1000 классов, результаты действительно получились достаточно высокими). Если посмотреть на все результаты в приложении, то можно увидеть, что архитектура ResNet показывает достаточно низкий результат для косинусного расстояния: 0.52 для ResNet101 и 0.37 для ResNet152. Но несмотря на это, все остальные архитектуры показали результат выше 0.9 для евклидово расстояния, а также результат выше 0.7 для косинусного расстояния за исключением двух рассмотренных выше случаев. Самый лучший результат показывают модели EfiicientNet, они получили результат выше 0.9 и для косинусного, и для евклидого расстояния, причем при увеличении параметров модели точность увеличивалась, таким образом, EfficintNetB7 показывает значение точности 0.98  и для косинусного, и для евклидово расстояния.

Теперь мы можем попробовать сравнить разные нейросети. Для начала нейросети можно сравнить, используя непосредственно сами значения в матрицах $\mathbb{O}$ и $\mathbb{W}$. Полные данные приведены в приложении, здесь приведем лишь три нейросети.

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Нейронная сеть & Среднее значение матрицы $\mathbb{W}$ & Среднее значение матрицы $\mathbb{O}$ \\ 
 \hline
 \hline
 VGG16 & -5.5e-7 & 0.67 \\
 \hline
 MobileNetV2 & -3.2e-5 & 0.92 \\
 \hline
 ResNet50 & 3.7e-7 & 0.55 \\
 \hline
\end{tabular}
\end{center}

Как видно из таблицы, средние значения матрицы $\mathbb{W}$ получаются достаточно близкие к нулю вне зависимости от архитектуры нейросети, в то время как средние значения матрицы $\mathbb{O}$ варьируются от архитектуры нейросети: для VGG они равны примерно 0.6, для MobileNetV2 - примерно 0.9, для InceptionV3 - примерно 0.6, для ResNet при увеличении числа слоев увеличивается среднее значение $\mathbb{O}$, то есть для ResNet50 среднее значение равно 0.5, для ResNet101 - 40.1, а для ResNet152 - 94, для DenseNet среднее значение варьируется около 1.5, а для EfficientNet средние значения лежит около 0, при этом при увеличении числа параметров средние значения $\mathbb{O}$ уменьшается.

\bibliographystyle{unsrtnat}
\bibliography{references}
\newpage

\section{Приложение}

Приведем полную таблицу результатов для всех архитектур нейронных сетей, на которых проводились эксперименты. В таблице приведена точность (accuracy) между предсказанием нейронной сети и предсказанием KNN, в котором объекты и классы находятся в одном пространстве и представлены матрицами $\mathbb{O}$ и $\mathbb{W}$.

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Нейронная сеть & Евклидово расстояние & Косинусное расстояние \\ 
 \hline
 \hline
 VGG16 & 0.94 & 0.84 \\
 \hline
 VGG19 & 0.94 & 0.84 \\
 \hline
 MobileNetV2 & 0.93 & 0.92 \\
 \hline
 ResNet50 & 0.96 & 0.82 \\
 \hline
 ResNet101 & 0.99 & 0.52 \\
 \hline
 ResNet152 & 0.99 & 0.37 \\
 \hline
 DenseNet121 & 0.98 & 0.74 \\
 \hline
 DenseNet169 & 0.99 & 0.83 \\
 \hline
 DenseNet201 & 0.98 & 0.73 \\
 \hline
 InceptionV3 & 0.99 & 0.76 \\
 \hline
 EfficientNetB0 & 0.94 & 0.93 \\
 \hline
 EfficientNetB1 & 0.96 & 0.94 \\
 \hline
 EfficientNetB2 & 0.96 & 0.95 \\
 \hline
 EfficientNetB3 & 0.97 & 0.96 \\
 \hline
 EfficientNetB4 & 0.98 & 0.97 \\
 \hline
 EfficientNetB5 & 0.98 & 0.97 \\
 \hline
 EfficientNetB6 & 0.98 & 0.97 \\
 \hline
 EfficientNetB7 & 0.98 & 0.98 \\
 \hline
\end{tabular}
\end{center}



В таблице приведены средние значения для матрицы $\mathbb{W}$ и матрицы $\mathbb{O}$ для всех архитектур нейронных сетей

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Нейронная сеть & Среднее значение матрицы $\mathbb{W}$ & Среднее значение матрицы $\mathbb{O}$ \\ 
 \hline
 \hline
 VGG16 & -5.5e-7 & 0.67 \\
 \hline
 VGG19 & -7.1e-7 & 0.62 \\
 \hline
 MobileNetV2 & -3.2e-5 & 0.92 \\
 \hline
 ResNet50 & 3.7e-7 & 0.55 \\
 \hline
 ResNet101 & 6.8e-6 & 40.11 \\
 \hline
 ResNet152 & 1.2e-5 & 93.98 \\
 \hline
 DenseNet121 & 2.2e-8 & 1.73 \\
 \hline
 DenseNet169 & 1.5e-8 & 1.66 \\
 \hline
 DenseNet201 & 1.1e-8 & 1.21 \\
 \hline
 InceptionV3 & -3.9e-5 & 0.61 \\
 \hline
 EfficientNetB0 & -7.5e-5 & 0.065 \\
 \hline
 EfficientNetB1 & -5.6e-5 & 0.056 \\
 \hline
 EfficientNetB2 & -7.2e-5 & 0.021 \\
 \hline
 EfficientNetB3 & -6.3e-5 & 0.011 \\
 \hline
 EfficientNetB4 & -3.1e-5 & -0.04 \\
 \hline
 EfficientNetB5 & -2.1e-5 & -0.039 \\
 \hline
 EfficientNetB6 & -8.6e-6 & -0.045 \\
 \hline
 EfficientNetB7 & -9.5e-6 & -0.043 \\
 \hline
\end{tabular}
\end{center}

\end{document}
